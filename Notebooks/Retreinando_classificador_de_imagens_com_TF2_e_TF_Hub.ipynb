{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Retreinando classificador de imagens com TF2 e TF Hub",
      "provenance": [],
      "collapsed_sections": [
        "ScitaPqhKtuW"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21UTUqA1LiKT",
        "colab_type": "text"
      },
      "source": [
        "> Esse notebook faz referências ao curso gratuíto 'Intro to TensorFlow for Deep Learning' da UDACITY (https://classroom.udacity.com/courses/ud187/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYM61xrTsP5d"
      },
      "source": [
        "# Retreinando classificador de imagens com TF2 e TF Hub\n",
        "\n",
        "\u003ctable align=\"left\"\u003e\n",
        "\u003ctd align=\"center\"\u003e\n",
        "  \u003ca target=\"_blank\"  href=\"https://colab.research.google.com/github/dntxos/TFKIT201/blob/master/Notebooks/Retreinando_classificador_de_imagens_com_TF2_e_TF_Hub.ipynb\"\u003e\n",
        "    \u003cimg src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" /\u003e\u003cbr\u003eRun in Google Colab\n",
        "  \u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "\u003ctd align=\"center\"\u003e\n",
        "  \u003ca target=\"_blank\"  href=\"https://github.com/dntxos/TFKIT201/blob/master/Notebooks/Retreinando_classificador_de_imagens_com_TF2_e_TF_Hub.ipynb\"\u003e\n",
        "    \u003cimg width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" /\u003e\u003cbr\u003eView source on GitHub\u003c/a\u003e\n",
        "\u003c/td\u003e\n",
        "\u003c/table\u003e"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L1otmJgmbahf"
      },
      "source": [
        "## Introdução\n",
        "\n",
        "Modelos classificadores de imagens possuem milhões de parâmetros. Treinar um modelo a partir\n",
        "do ZERO requer um enorme volume de dados anotados e muito poder computacional. Transferência de aprendizado (transfer learning)\n",
        "é uma técnica que permite encurtar esse caminho utilizando modelos já treinados como Inception ou MobileNet reaproveitando\n",
        "todo o aprendizado e poder computacional gasto para treinar esses modelos, treinando apenas as últimas camadas da rede.\n",
        "\n",
        "Video explicativo do processo: https://youtu.be/JhR1_WZCj54\n",
        "\n",
        "### Procurando pela ferramenta para criar um modelo classificador de imagens?\n",
        "\n",
        "Este é um notebook tutorial. Se você quer uma ferramenta que apenas compila um modelo TensorFlow ou TF Lite, dê uma olhada na ferramenta de linha de comando [make_image_classifier](https://github.com/tensorflow/hub/tree/master/tensorflow_hub/tools/make_image_classifier), que pode ser [instalada](https://www.tensorflow.org/hub/installation) através do PIP `tensorflow-hub[make_image_classifier]`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bL54LWCHt5q5"
      },
      "source": [
        "## Configurando TensorFlow 2 e outras bibliotecas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwjjM_-DzDNh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Desconsidere esse bloco no caso de executar em 'Colab Hosted Runtime'!\n",
        "\n",
        "# Considerando que o container seja baseado na imagem docker do Tensorflow 2.0.1, os pacotes abaixo precisam ser instalados:\n",
        "\n",
        "!pip -q --disable-pip-version-check install pillow\n",
        "!pip -q --disable-pip-version-check install scipy\n",
        "!pip -q --disable-pip-version-check install tensorflow_hub\n",
        "\n",
        "# Caso tenha problemas na importação das bibliotecas, reinicie o Kernel."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "txdrFJrJvgQS",
        "colab": {}
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version EXISTE SOMENTE NO COLAB\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dlauq-4FWGZM",
        "colab": {}
      },
      "source": [
        "import itertools\n",
        "import os\n",
        "\n",
        "import matplotlib.pylab as plt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "print(\"Versão do TensorFlow:\", tf.__version__)\n",
        "print(\"Versão do TF Hub:\", hub.__version__)\n",
        "print(\"GPU\", \"DISPONÍVEL\" if len(tf.config.list_physical_devices('GPU')) else \"NÃO DISPONÍVEL\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mmaHHH7Pvmth"
      },
      "source": [
        "## Selecione o módulo (TF2 SavedModel) para utilizar no retreino\n",
        "\n",
        "Para iniciantes, use https://tfhub.dev/google/imagenet/mobilenet_v2_100_224/feature_vector/4. Você também pode acessar essa mesma URL para consultar a documentação do modelo no TFHUB. (`hub.Modules` para TF 1.x não funciona aqui.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FlsEcKVeuCnf",
        "colab": {}
      },
      "source": [
        "#@markdown Escolha um dos modelos da lista abaixo, ou entre com o nome do modelo `image-feature-vector` disponível para TF2 na página do TFHUB (https://tfhub.dev/s?module-type=image-feature-vector&tf-version=tf2).\n",
        "module_selection = (\"mobilenet_v2_100_224\", 224) #@param [\"(\\\"mobilenet_v2_100_224\\\", 224)\", \"(\\\"inception_v3\\\", 299)\"] {type:\"raw\", allow-input: true}\n",
        "handle_base, pixels = module_selection\n",
        "MODULE_HANDLE =\"https://tfhub.dev/google/imagenet/{}/feature_vector/4\".format(handle_base)\n",
        "IMAGE_SIZE = (pixels, pixels)\n",
        "print(\"Utilizando {} com tamanho de entrada {} pixels\".format(MODULE_HANDLE, IMAGE_SIZE))\n",
        "\n",
        "#@markdown Defina o tamanho do lote de imagens para o treinamento. Quanto maior o tamanho do lote, mais memória será demandada ao treinamento.\n",
        "BATCH_SIZE = 32 #@param {type:\"integer\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yTY8qzyYv3vl"
      },
      "source": [
        "## Configurando o Dataset (Flowers)\n",
        "\n",
        "As entradas são redimensionadas adequadamente para o módulo selecionado. E aumento do conjunto de dados (*Data augmentation, ou seja, distorções aleatórias de uma imagem cada vez que é lida) melhora o treinamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WBtFK1hO8KsO",
        "colab": {}
      },
      "source": [
        "data_dir = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "umB5tswsfTEQ",
        "colab": {}
      },
      "source": [
        "datagen_kwargs = dict(rescale=1./255, validation_split=.20)\n",
        "dataflow_kwargs = dict(target_size=IMAGE_SIZE, batch_size=BATCH_SIZE,\n",
        "                   interpolation=\"bilinear\")\n",
        "\n",
        "valid_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    **datagen_kwargs)\n",
        "valid_generator = valid_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"validation\", shuffle=False, **dataflow_kwargs)\n",
        "\n",
        "#@markdown Ative `do_data_augmentation`para aumentar artificialmente o número de imagens em nosso conjunto de treinamento aplicando transformações aleatórias de imagem nas imagens existentes no conjunto de treinamento. É muito útil para datasets muito pequenos e ajuda a generalizar. (Video: https://youtu.be/Qgd7maIVytI)\n",
        "do_data_augmentation = False #@param {type:\"boolean\"}\n",
        "if do_data_augmentation:\n",
        "  train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "      rotation_range=40,\n",
        "      horizontal_flip=True,\n",
        "      width_shift_range=0.2, height_shift_range=0.2,\n",
        "      shear_range=0.2, zoom_range=0.2,\n",
        "      **datagen_kwargs)\n",
        "else:\n",
        "  train_datagen = valid_datagen\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    data_dir, subset=\"training\", shuffle=True, **dataflow_kwargs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FS_gVStowW3G"
      },
      "source": [
        "\n",
        "## Definindo o modelo\n",
        "\n",
        "Basta colocar um classificador linear em cima do `feature_extractor_layer` com módulo do TFHUB.\n",
        "\n",
        "Para mais velocidade, começamos com um `feature_extractor_layer` não treinável, mas você também pode ativar o ajuste fino (fine-tuning) para obter maior precisão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RaJW3XrPyFiF",
        "colab": {}
      },
      "source": [
        "do_fine_tuning = False #@param {type:\"boolean\"}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "50FYNIb1dmJH",
        "colab": {}
      },
      "source": [
        "print(\"Compilando o modelo com\", MODULE_HANDLE)\n",
        "model = tf.keras.Sequential([\n",
        "    hub.KerasLayer(MODULE_HANDLE, trainable=do_fine_tuning),\n",
        "    tf.keras.layers.Dropout(rate=0.2),\n",
        "    tf.keras.layers.Dense(train_generator.num_classes, activation='softmax',\n",
        "                          kernel_regularizer=tf.keras.regularizers.l2(0.0001))\n",
        "])\n",
        "model.build((None,)+IMAGE_SIZE+(3,))\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "u2e5WupIw2N2"
      },
      "source": [
        "## Treinando o modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9f3yBUvkd_VJ",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9), \n",
        "  loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.1),\n",
        "  metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "w_YKX2Qnfg6x",
        "colab": {}
      },
      "source": [
        "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
        "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
        "hist = model.fit_generator(\n",
        "    train_generator,\n",
        "    epochs=5, steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=valid_generator,\n",
        "    validation_steps=validation_steps).history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CYOw0fTO1W4x",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "plt.ylabel(\"Loss (Treinamento e validação)\")\n",
        "plt.xlabel(\"Passos do treinamento\")\n",
        "plt.ylim([0,2])\n",
        "plt.plot(hist[\"loss\"])\n",
        "plt.plot(hist[\"val_loss\"])\n",
        "\n",
        "plt.figure()\n",
        "plt.ylabel(\"Acurácia (Treinamento e validação)\")\n",
        "plt.xlabel(\"Passos do treinamento\")\n",
        "plt.ylim([0,1])\n",
        "plt.plot(hist[\"accuracy\"])\n",
        "plt.plot(hist[\"val_accuracy\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YCsAsQM1IRvA"
      },
      "source": [
        "Finalmente, seu modelo pode ser salvo para implantação no TFX/Tensorflow_Serving ou para dispositivos móveis com TF Lite a seguir.\n",
        "\n",
        "Visualizações da sua rede pode te ajudar a entender ainda mais (https://towardsdatascience.com/understanding-your-convolution-network-with-visualizations-a4883441533b)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LGvTi69oIc2d",
        "colab": {}
      },
      "source": [
        "saved_model_path = \"/tmp/saved_flowers_model\"\n",
        "tf.saved_model.save(model, saved_model_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QzW4oNRjILaq"
      },
      "source": [
        "## Opcional: Implantação em TensorFlow Lite\n",
        "\n",
        "[TensorFlow Lite](https://www.tensorflow.org/lite) permite implantar modelos TensorFlow em dispositivos móveis e IoT. O código abaixo demonstra como converter o modelo treinado para o formato TF Lite e utilizar ferramentas pós-treinamento do [Tensorflow Model Optimization Toolkit](https://www.tensorflow.org/model_optimization). Ao final, executamos no 'Interpretador TF Lite' para examinar a qualidade dos resultados\n",
        "\n",
        "  * A conversão sem otimização fornece os mesmos resultados de antes (até o erro de arredondamento).\n",
        "  * A conversão com otimização sem dados quantifica os pesos do modelo para 8 bits, mas a inferência ainda usa computação em ponto flutuante para as ativações da rede neural. Isso reduz o tamanho do modelo quase por um fator de 4 e melhora a latência da CPU em dispositivos móveis.\n",
        "  * Além disso, o cálculo das ativações da rede neural pode ser quantizado para números inteiros de 8 bits, se um pequeno conjunto de dados de referência for fornecido para calibrar o intervalo de quantização. Em um dispositivo móvel, isso acelera ainda mais a inferência e possibilita a execução em aceleradores como o EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Va1Vo92fSyV6",
        "colab": {}
      },
      "source": [
        "#@title Configurações da otimização\n",
        "optimize_lite_model = False  #@param {type:\"boolean\"}\n",
        "#@markdown Definir um valor maior que zero permite a quantização de ativações de redes neurais. Algumas dezenas já são uma quantia útil.\n",
        "num_calibration_examples = 60  #@param {type:\"slider\", min:0, max:1000, step:1}\n",
        "representative_dataset = None\n",
        "if optimize_lite_model and num_calibration_examples:\n",
        "  # Use a bounded number of training examples without labels for calibration.\n",
        "  # TFLiteConverter expects a list of input tensors, each with batch size 1.\n",
        "  representative_dataset = lambda: itertools.islice(\n",
        "      ([image[None, ...]] for batch, _ in train_generator for image in batch),\n",
        "      num_calibration_examples)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_path)\n",
        "if optimize_lite_model:\n",
        "  converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "  if representative_dataset:  # This is optional, see above.\n",
        "    converter.representative_dataset = representative_dataset\n",
        "lite_model_content = converter.convert()\n",
        "\n",
        "with open(\"/tmp/lite_flowers_model\", \"wb\") as f:\n",
        "  f.write(lite_model_content)\n",
        "print(\"Wrote %sTFLite model of %d bytes.\" %\n",
        "      (\"optimized \" if optimize_lite_model else \"\", len(lite_model_content)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_wqEmD0xIqeG",
        "colab": {}
      },
      "source": [
        "interpreter = tf.lite.Interpreter(model_content=lite_model_content)\n",
        "# This little helper wraps the TF Lite interpreter as a numpy-to-numpy function.\n",
        "def lite_model(images):\n",
        "  interpreter.allocate_tensors()\n",
        "  interpreter.set_tensor(interpreter.get_input_details()[0]['index'], images)\n",
        "  interpreter.invoke()\n",
        "  return interpreter.get_tensor(interpreter.get_output_details()[0]['index'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JMMK-fZrKrk8",
        "colab": {}
      },
      "source": [
        "#@markdown Para experimentação rápida, comece com um número moderado de exemplos.\n",
        "num_eval_examples = 50  #@param {type:\"slider\", min:0, max:700}\n",
        "eval_dataset = ((image, label)  # TFLite expects batch size 1.\n",
        "                for batch in train_generator\n",
        "                for (image, label) in zip(*batch))\n",
        "count = 0\n",
        "count_lite_tf_agree = 0\n",
        "count_lite_correct = 0\n",
        "for image, label in eval_dataset:\n",
        "  probs_lite = lite_model(image[None, ...])[0]\n",
        "  probs_tf = model(image[None, ...]).numpy()[0]\n",
        "  y_lite = np.argmax(probs_lite)\n",
        "  y_tf = np.argmax(probs_tf)\n",
        "  y_true = np.argmax(label)\n",
        "  count +=1\n",
        "  if y_lite == y_tf: count_lite_tf_agree += 1\n",
        "  if y_lite == y_true: count_lite_correct += 1\n",
        "  if count >= num_eval_examples: break\n",
        "print(\"TF Lite model agrees with original model on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_tf_agree, count, 100.0 * count_lite_tf_agree / count))\n",
        "print(\"TF Lite model is accurate on %d of %d examples (%g%%).\" %\n",
        "      (count_lite_correct, count, 100.0 * count_lite_correct / count))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PolHvzi0Ufe",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ScitaPqhKtuW"
      },
      "source": [
        "Este notebook é baseado no [TF Hub for TF2: Retraining an image classifier](https://github.com/tensorflow/hub/blob/master/examples/colab/tf2_image_retraining.ipynb), adaptado para rodar localmente, sem código deprecado, traduzido para o português com links para vídeos do curso gratuíto 'ud187 - Intro to TensorFlow for Deep Learning' da UDACITY (https://classroom.udacity.com/courses/ud187/)\n",
        "\n",
        "##### Copyright 2019 The TensorFlow Hub Authors.\n",
        "Licensed under the Apache License, Version 2.0 (the \"License\");"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jvztxQ6VsK2k",
        "colab": {}
      },
      "source": [
        "# Copyright 2019 The TensorFlow Hub Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# =============================================================================="
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
